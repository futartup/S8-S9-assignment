{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "QuizDNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/futartup/S8-S9-assignment/blob/master/QuizDNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T18X5JljDgdH",
        "colab_type": "code",
        "outputId": "208e7066-6d19-4618-8dc5-641ecc0193a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from datetime import datetime\n",
        "print(\"Current Date/Time: \", datetime.now())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Date/Time:  2020-04-01 15:08:19.299454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71sESFSnEHE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "import sys\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-w9C_aTQ8_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78063446-e045-402f-e6c0-e86e13a9e3f2"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, nChannels, growthRate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        interChannels = 4*growthRate\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
        "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat((x, out), 1)\n",
        "        return out\n",
        "\n",
        "class SingleLayer(nn.Module):\n",
        "    def __init__(self, nChannels, growthRate):\n",
        "        super(SingleLayer, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = torch.cat((x, out), 1)\n",
        "        return out\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, nChannels, nOutChannels):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
        "                               bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        nDenseBlocks = (depth-4) // 3\n",
        "        if bottleneck:\n",
        "            nDenseBlocks //= 2\n",
        "\n",
        "        nChannels = 2*growthRate\n",
        "        self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "        nOutChannels = int(math.floor(nChannels*reduction))\n",
        "        self.trans1 = Transition(nChannels, nOutChannels)\n",
        "\n",
        "        nChannels = nOutChannels\n",
        "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "        nOutChannels = int(math.floor(nChannels*reduction))\n",
        "        self.trans2 = Transition(nChannels, nOutChannels)\n",
        "\n",
        "        nChannels = nOutChannels\n",
        "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
        "        nChannels += nDenseBlocks*growthRate\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
        "        self.fc = nn.Linear(nChannels, nClasses)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
        "        layers = []\n",
        "        for i in range(int(nDenseBlocks)):\n",
        "            if bottleneck:\n",
        "                layers.append(Bottleneck(nChannels, growthRate))\n",
        "            else:\n",
        "                layers.append(SingleLayer(nChannels, growthRate))\n",
        "            nChannels += growthRate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.dense1(out))\n",
        "        out = self.trans2(self.dense2(out))\n",
        "        out = self.dense3(out)\n",
        "        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
        "        out = F.log_softmax(self.fc(out))\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 24, 32, 32]             648\n",
            "       BatchNorm2d-2           [-1, 24, 32, 32]              48\n",
            "            Conv2d-3           [-1, 48, 32, 32]           1,152\n",
            "       BatchNorm2d-4           [-1, 48, 32, 32]              96\n",
            "            Conv2d-5           [-1, 12, 32, 32]           5,184\n",
            "        Bottleneck-6           [-1, 36, 32, 32]               0\n",
            "       BatchNorm2d-7           [-1, 36, 32, 32]              72\n",
            "            Conv2d-8           [-1, 48, 32, 32]           1,728\n",
            "       BatchNorm2d-9           [-1, 48, 32, 32]              96\n",
            "           Conv2d-10           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-11           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-12           [-1, 48, 32, 32]              96\n",
            "           Conv2d-13           [-1, 48, 32, 32]           2,304\n",
            "      BatchNorm2d-14           [-1, 48, 32, 32]              96\n",
            "           Conv2d-15           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-16           [-1, 60, 32, 32]               0\n",
            "      BatchNorm2d-17           [-1, 60, 32, 32]             120\n",
            "           Conv2d-18           [-1, 48, 32, 32]           2,880\n",
            "      BatchNorm2d-19           [-1, 48, 32, 32]              96\n",
            "           Conv2d-20           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-21           [-1, 72, 32, 32]               0\n",
            "      BatchNorm2d-22           [-1, 72, 32, 32]             144\n",
            "           Conv2d-23           [-1, 48, 32, 32]           3,456\n",
            "      BatchNorm2d-24           [-1, 48, 32, 32]              96\n",
            "           Conv2d-25           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-26           [-1, 84, 32, 32]               0\n",
            "      BatchNorm2d-27           [-1, 84, 32, 32]             168\n",
            "           Conv2d-28           [-1, 48, 32, 32]           4,032\n",
            "      BatchNorm2d-29           [-1, 48, 32, 32]              96\n",
            "           Conv2d-30           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-31           [-1, 96, 32, 32]               0\n",
            "      BatchNorm2d-32           [-1, 96, 32, 32]             192\n",
            "           Conv2d-33           [-1, 48, 32, 32]           4,608\n",
            "      BatchNorm2d-34           [-1, 48, 32, 32]              96\n",
            "           Conv2d-35           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-36          [-1, 108, 32, 32]               0\n",
            "      BatchNorm2d-37          [-1, 108, 32, 32]             216\n",
            "           Conv2d-38           [-1, 48, 32, 32]           5,184\n",
            "      BatchNorm2d-39           [-1, 48, 32, 32]              96\n",
            "           Conv2d-40           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-41          [-1, 120, 32, 32]               0\n",
            "      BatchNorm2d-42          [-1, 120, 32, 32]             240\n",
            "           Conv2d-43           [-1, 48, 32, 32]           5,760\n",
            "      BatchNorm2d-44           [-1, 48, 32, 32]              96\n",
            "           Conv2d-45           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-46          [-1, 132, 32, 32]               0\n",
            "      BatchNorm2d-47          [-1, 132, 32, 32]             264\n",
            "           Conv2d-48           [-1, 48, 32, 32]           6,336\n",
            "      BatchNorm2d-49           [-1, 48, 32, 32]              96\n",
            "           Conv2d-50           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-51          [-1, 144, 32, 32]               0\n",
            "      BatchNorm2d-52          [-1, 144, 32, 32]             288\n",
            "           Conv2d-53           [-1, 48, 32, 32]           6,912\n",
            "      BatchNorm2d-54           [-1, 48, 32, 32]              96\n",
            "           Conv2d-55           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-56          [-1, 156, 32, 32]               0\n",
            "      BatchNorm2d-57          [-1, 156, 32, 32]             312\n",
            "           Conv2d-58           [-1, 48, 32, 32]           7,488\n",
            "      BatchNorm2d-59           [-1, 48, 32, 32]              96\n",
            "           Conv2d-60           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-61          [-1, 168, 32, 32]               0\n",
            "      BatchNorm2d-62          [-1, 168, 32, 32]             336\n",
            "           Conv2d-63           [-1, 48, 32, 32]           8,064\n",
            "      BatchNorm2d-64           [-1, 48, 32, 32]              96\n",
            "           Conv2d-65           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-66          [-1, 180, 32, 32]               0\n",
            "      BatchNorm2d-67          [-1, 180, 32, 32]             360\n",
            "           Conv2d-68           [-1, 48, 32, 32]           8,640\n",
            "      BatchNorm2d-69           [-1, 48, 32, 32]              96\n",
            "           Conv2d-70           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-71          [-1, 192, 32, 32]               0\n",
            "      BatchNorm2d-72          [-1, 192, 32, 32]             384\n",
            "           Conv2d-73           [-1, 48, 32, 32]           9,216\n",
            "      BatchNorm2d-74           [-1, 48, 32, 32]              96\n",
            "           Conv2d-75           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-76          [-1, 204, 32, 32]               0\n",
            "      BatchNorm2d-77          [-1, 204, 32, 32]             408\n",
            "           Conv2d-78           [-1, 48, 32, 32]           9,792\n",
            "      BatchNorm2d-79           [-1, 48, 32, 32]              96\n",
            "           Conv2d-80           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-81          [-1, 216, 32, 32]               0\n",
            "      BatchNorm2d-82          [-1, 216, 32, 32]             432\n",
            "           Conv2d-83          [-1, 108, 32, 32]          23,328\n",
            "       Transition-84          [-1, 108, 16, 16]               0\n",
            "      BatchNorm2d-85          [-1, 108, 16, 16]             216\n",
            "           Conv2d-86           [-1, 48, 16, 16]           5,184\n",
            "      BatchNorm2d-87           [-1, 48, 16, 16]              96\n",
            "           Conv2d-88           [-1, 12, 16, 16]           5,184\n",
            "       Bottleneck-89          [-1, 120, 16, 16]               0\n",
            "      BatchNorm2d-90          [-1, 120, 16, 16]             240\n",
            "           Conv2d-91           [-1, 48, 16, 16]           5,760\n",
            "      BatchNorm2d-92           [-1, 48, 16, 16]              96\n",
            "           Conv2d-93           [-1, 12, 16, 16]           5,184\n",
            "       Bottleneck-94          [-1, 132, 16, 16]               0\n",
            "      BatchNorm2d-95          [-1, 132, 16, 16]             264\n",
            "           Conv2d-96           [-1, 48, 16, 16]           6,336\n",
            "      BatchNorm2d-97           [-1, 48, 16, 16]              96\n",
            "           Conv2d-98           [-1, 12, 16, 16]           5,184\n",
            "       Bottleneck-99          [-1, 144, 16, 16]               0\n",
            "     BatchNorm2d-100          [-1, 144, 16, 16]             288\n",
            "          Conv2d-101           [-1, 48, 16, 16]           6,912\n",
            "     BatchNorm2d-102           [-1, 48, 16, 16]              96\n",
            "          Conv2d-103           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-104          [-1, 156, 16, 16]               0\n",
            "     BatchNorm2d-105          [-1, 156, 16, 16]             312\n",
            "          Conv2d-106           [-1, 48, 16, 16]           7,488\n",
            "     BatchNorm2d-107           [-1, 48, 16, 16]              96\n",
            "          Conv2d-108           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-109          [-1, 168, 16, 16]               0\n",
            "     BatchNorm2d-110          [-1, 168, 16, 16]             336\n",
            "          Conv2d-111           [-1, 48, 16, 16]           8,064\n",
            "     BatchNorm2d-112           [-1, 48, 16, 16]              96\n",
            "          Conv2d-113           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-114          [-1, 180, 16, 16]               0\n",
            "     BatchNorm2d-115          [-1, 180, 16, 16]             360\n",
            "          Conv2d-116           [-1, 48, 16, 16]           8,640\n",
            "     BatchNorm2d-117           [-1, 48, 16, 16]              96\n",
            "          Conv2d-118           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-119          [-1, 192, 16, 16]               0\n",
            "     BatchNorm2d-120          [-1, 192, 16, 16]             384\n",
            "          Conv2d-121           [-1, 48, 16, 16]           9,216\n",
            "     BatchNorm2d-122           [-1, 48, 16, 16]              96\n",
            "          Conv2d-123           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-124          [-1, 204, 16, 16]               0\n",
            "     BatchNorm2d-125          [-1, 204, 16, 16]             408\n",
            "          Conv2d-126           [-1, 48, 16, 16]           9,792\n",
            "     BatchNorm2d-127           [-1, 48, 16, 16]              96\n",
            "          Conv2d-128           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-129          [-1, 216, 16, 16]               0\n",
            "     BatchNorm2d-130          [-1, 216, 16, 16]             432\n",
            "          Conv2d-131           [-1, 48, 16, 16]          10,368\n",
            "     BatchNorm2d-132           [-1, 48, 16, 16]              96\n",
            "          Conv2d-133           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-134          [-1, 228, 16, 16]               0\n",
            "     BatchNorm2d-135          [-1, 228, 16, 16]             456\n",
            "          Conv2d-136           [-1, 48, 16, 16]          10,944\n",
            "     BatchNorm2d-137           [-1, 48, 16, 16]              96\n",
            "          Conv2d-138           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-139          [-1, 240, 16, 16]               0\n",
            "     BatchNorm2d-140          [-1, 240, 16, 16]             480\n",
            "          Conv2d-141           [-1, 48, 16, 16]          11,520\n",
            "     BatchNorm2d-142           [-1, 48, 16, 16]              96\n",
            "          Conv2d-143           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-144          [-1, 252, 16, 16]               0\n",
            "     BatchNorm2d-145          [-1, 252, 16, 16]             504\n",
            "          Conv2d-146           [-1, 48, 16, 16]          12,096\n",
            "     BatchNorm2d-147           [-1, 48, 16, 16]              96\n",
            "          Conv2d-148           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-149          [-1, 264, 16, 16]               0\n",
            "     BatchNorm2d-150          [-1, 264, 16, 16]             528\n",
            "          Conv2d-151           [-1, 48, 16, 16]          12,672\n",
            "     BatchNorm2d-152           [-1, 48, 16, 16]              96\n",
            "          Conv2d-153           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-154          [-1, 276, 16, 16]               0\n",
            "     BatchNorm2d-155          [-1, 276, 16, 16]             552\n",
            "          Conv2d-156           [-1, 48, 16, 16]          13,248\n",
            "     BatchNorm2d-157           [-1, 48, 16, 16]              96\n",
            "          Conv2d-158           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-159          [-1, 288, 16, 16]               0\n",
            "     BatchNorm2d-160          [-1, 288, 16, 16]             576\n",
            "          Conv2d-161           [-1, 48, 16, 16]          13,824\n",
            "     BatchNorm2d-162           [-1, 48, 16, 16]              96\n",
            "          Conv2d-163           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-164          [-1, 300, 16, 16]               0\n",
            "     BatchNorm2d-165          [-1, 300, 16, 16]             600\n",
            "          Conv2d-166          [-1, 150, 16, 16]          45,000\n",
            "      Transition-167            [-1, 150, 8, 8]               0\n",
            "     BatchNorm2d-168            [-1, 150, 8, 8]             300\n",
            "          Conv2d-169             [-1, 48, 8, 8]           7,200\n",
            "     BatchNorm2d-170             [-1, 48, 8, 8]              96\n",
            "          Conv2d-171             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-172            [-1, 162, 8, 8]               0\n",
            "     BatchNorm2d-173            [-1, 162, 8, 8]             324\n",
            "          Conv2d-174             [-1, 48, 8, 8]           7,776\n",
            "     BatchNorm2d-175             [-1, 48, 8, 8]              96\n",
            "          Conv2d-176             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-177            [-1, 174, 8, 8]               0\n",
            "     BatchNorm2d-178            [-1, 174, 8, 8]             348\n",
            "          Conv2d-179             [-1, 48, 8, 8]           8,352\n",
            "     BatchNorm2d-180             [-1, 48, 8, 8]              96\n",
            "          Conv2d-181             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-182            [-1, 186, 8, 8]               0\n",
            "     BatchNorm2d-183            [-1, 186, 8, 8]             372\n",
            "          Conv2d-184             [-1, 48, 8, 8]           8,928\n",
            "     BatchNorm2d-185             [-1, 48, 8, 8]              96\n",
            "          Conv2d-186             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-187            [-1, 198, 8, 8]               0\n",
            "     BatchNorm2d-188            [-1, 198, 8, 8]             396\n",
            "          Conv2d-189             [-1, 48, 8, 8]           9,504\n",
            "     BatchNorm2d-190             [-1, 48, 8, 8]              96\n",
            "          Conv2d-191             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-192            [-1, 210, 8, 8]               0\n",
            "     BatchNorm2d-193            [-1, 210, 8, 8]             420\n",
            "          Conv2d-194             [-1, 48, 8, 8]          10,080\n",
            "     BatchNorm2d-195             [-1, 48, 8, 8]              96\n",
            "          Conv2d-196             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-197            [-1, 222, 8, 8]               0\n",
            "     BatchNorm2d-198            [-1, 222, 8, 8]             444\n",
            "          Conv2d-199             [-1, 48, 8, 8]          10,656\n",
            "     BatchNorm2d-200             [-1, 48, 8, 8]              96\n",
            "          Conv2d-201             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-202            [-1, 234, 8, 8]               0\n",
            "     BatchNorm2d-203            [-1, 234, 8, 8]             468\n",
            "          Conv2d-204             [-1, 48, 8, 8]          11,232\n",
            "     BatchNorm2d-205             [-1, 48, 8, 8]              96\n",
            "          Conv2d-206             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-207            [-1, 246, 8, 8]               0\n",
            "     BatchNorm2d-208            [-1, 246, 8, 8]             492\n",
            "          Conv2d-209             [-1, 48, 8, 8]          11,808\n",
            "     BatchNorm2d-210             [-1, 48, 8, 8]              96\n",
            "          Conv2d-211             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-212            [-1, 258, 8, 8]               0\n",
            "     BatchNorm2d-213            [-1, 258, 8, 8]             516\n",
            "          Conv2d-214             [-1, 48, 8, 8]          12,384\n",
            "     BatchNorm2d-215             [-1, 48, 8, 8]              96\n",
            "          Conv2d-216             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-217            [-1, 270, 8, 8]               0\n",
            "     BatchNorm2d-218            [-1, 270, 8, 8]             540\n",
            "          Conv2d-219             [-1, 48, 8, 8]          12,960\n",
            "     BatchNorm2d-220             [-1, 48, 8, 8]              96\n",
            "          Conv2d-221             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-222            [-1, 282, 8, 8]               0\n",
            "     BatchNorm2d-223            [-1, 282, 8, 8]             564\n",
            "          Conv2d-224             [-1, 48, 8, 8]          13,536\n",
            "     BatchNorm2d-225             [-1, 48, 8, 8]              96\n",
            "          Conv2d-226             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-227            [-1, 294, 8, 8]               0\n",
            "     BatchNorm2d-228            [-1, 294, 8, 8]             588\n",
            "          Conv2d-229             [-1, 48, 8, 8]          14,112\n",
            "     BatchNorm2d-230             [-1, 48, 8, 8]              96\n",
            "          Conv2d-231             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-232            [-1, 306, 8, 8]               0\n",
            "     BatchNorm2d-233            [-1, 306, 8, 8]             612\n",
            "          Conv2d-234             [-1, 48, 8, 8]          14,688\n",
            "     BatchNorm2d-235             [-1, 48, 8, 8]              96\n",
            "          Conv2d-236             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-237            [-1, 318, 8, 8]               0\n",
            "     BatchNorm2d-238            [-1, 318, 8, 8]             636\n",
            "          Conv2d-239             [-1, 48, 8, 8]          15,264\n",
            "     BatchNorm2d-240             [-1, 48, 8, 8]              96\n",
            "          Conv2d-241             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-242            [-1, 330, 8, 8]               0\n",
            "     BatchNorm2d-243            [-1, 330, 8, 8]             660\n",
            "          Conv2d-244             [-1, 48, 8, 8]          15,840\n",
            "     BatchNorm2d-245             [-1, 48, 8, 8]              96\n",
            "          Conv2d-246             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-247            [-1, 342, 8, 8]               0\n",
            "     BatchNorm2d-248            [-1, 342, 8, 8]             684\n",
            "          Linear-249                   [-1, 10]           3,430\n",
            "================================================================\n",
            "Total params: 769,162\n",
            "Trainable params: 769,162\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 68.36\n",
            "Params size (MB): 2.93\n",
            "Estimated Total Size (MB): 71.31\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:115: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}